<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>G-Retriever f√ºr Obsidian Vault - Dokumentation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }

        header h1 {
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }

        header p {
            font-size: 1.3em;
            opacity: 0.95;
        }

        nav {
            background: #f8f9fa;
            padding: 20px 40px;
            border-bottom: 3px solid #667eea;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }

        nav a {
            color: #667eea;
            text-decoration: none;
            font-weight: 600;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #764ba2;
        }

        main {
            padding: 40px;
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            color: #667eea;
            font-size: 2.2em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }

        h3 {
            color: #764ba2;
            font-size: 1.6em;
            margin: 30px 0 15px 0;
        }

        h4 {
            color: #555;
            font-size: 1.3em;
            margin: 20px 0 10px 0;
        }

        p {
            margin-bottom: 15px;
        }

        ul, ol {
            margin: 15px 0 15px 30px;
        }

        li {
            margin-bottom: 8px;
        }

        .info-box {
            background: #e8f4f8;
            border-left: 5px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ff9800;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .success-box {
            background: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: #d63384;
        }

        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 5px solid #667eea;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        .module-card {
            background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 25px;
            margin: 20px 0;
            transition: transform 0.3s, box-shadow 0.3s;
        }

        .module-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.2);
        }

        .module-card h4 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.4em;
        }

        .pipeline-step {
            display: flex;
            align-items: flex-start;
            gap: 20px;
            margin: 30px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #667eea;
        }

        .step-number {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5em;
            font-weight: bold;
            flex-shrink: 0;
        }

        .step-content {
            flex: 1;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        td {
            padding: 12px 15px;
            border-bottom: 1px solid #ddd;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .architecture-diagram {
            background: white;
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 30px;
            margin: 30px 0;
            text-align: center;
        }

        .component {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 30px;
            margin: 10px;
            border-radius: 8px;
            font-weight: 600;
        }

        .arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }

        footer {
            background: #2d2d2d;
            color: white;
            text-align: center;
            padding: 30px;
            margin-top: 40px;
        }

        .btn {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            transition: transform 0.3s, box-shadow 0.3s;
            margin: 10px 5px;
        }

        .btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.4);
        }

        .comparison-table {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 30px 0;
        }

        .comparison-card {
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 25px;
            background: white;
        }

        .comparison-card h4 {
            color: #667eea;
            margin-bottom: 15px;
        }

        .comparison-card ul {
            list-style: none;
            padding-left: 0;
        }

        .comparison-card li {
            padding: 8px 0;
            padding-left: 25px;
            position: relative;
        }

        .comparison-card li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            color: #28a745;
            font-weight: bold;
        }

        @media (max-width: 768px) {
            .comparison-table {
                grid-template-columns: 1fr;
            }

            header h1 {
                font-size: 2em;
            }

            nav ul {
                flex-direction: column;
                gap: 10px;
            }

            .pipeline-step {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üß† G-Retriever f√ºr Obsidian</h1>
            <p>Verwandle dein Obsidian Vault in einen intelligenten, durchsuchbaren Wissensgraphen</p>
        </header>

        <nav>
            <ul>
                <li><a href="#overview">√úbersicht</a></li>
                <li><a href="#architecture">Architektur</a></li>
                <li><a href="#installation">Installation</a></li>
                <li><a href="#modules">Module</a></li>
                <li><a href="#workflow">Workflow</a></li>
                <li><a href="#training">Training</a></li>
                <li><a href="#usage">Nutzung</a></li>
                <li><a href="#comparison">Vergleich</a></li>
            </ul>
        </nav>

        <main>
            <section id="overview">
                <h2>üìñ √úbersicht</h2>
                <p>Dieses System verwandelt dein Obsidian Vault in einen durchsuchbaren Wissensgraphen mit Graph Neural Networks und Large Language Models. Es kombiniert moderne RAG (Retrieval-Augmented Generation) Techniken mit G-Retriever f√ºr pr√§zise Antworten auf Basis deiner pers√∂nlichen Notizen.</p>

                <div class="info-box">
                    <h4>üéØ Was macht das System?</h4>
                    <ul>
                        <li><strong>Graph-Konvertierung:</strong> Wandelt Markdown-Notizen in einen NetworkX-Graphen um</li>
                        <li><strong>QA-Generierung:</strong> Erstellt automatisch Frage-Antwort-Paare mit Ollama</li>
                        <li><strong>Smart Retrieval:</strong> Findet relevante Notizen mit Embeddings und Graph-Algorithmen</li>
                        <li><strong>Kontextuelle Antworten:</strong> Nutzt dein lokales LLM f√ºr pr√§zise Antworten</li>
                        <li><strong>Optional: GNN Training:</strong> Trainiert ein spezialisiertes Neural Network auf deinen Daten</li>
                    </ul>
                </div>

                <div class="architecture-diagram">
                    <h3>System-Architektur</h3>
                    <div>
                        <div class="component">Obsidian Vault</div>
                        <span class="arrow">‚Üí</span>
                        <div class="component">Graph Builder</div>
                        <span class="arrow">‚Üí</span>
                        <div class="component">Training Data</div>
                        <span class="arrow">‚Üí</span>
                        <div class="component">PyG Dataset</div>
                        <span class="arrow">‚Üí</span>
                        <div class="component">GNN Training</div>
                        <span class="arrow">‚Üí</span>
                        <div class="component">Chat Interface</div>
                    </div>
                </div>
            </section>

            <section id="architecture">
                <h2>üèóÔ∏è Technische Architektur</h2>

                <h3>Zwei Varianten verf√ºgbar:</h3>

                <div class="comparison-table">
                    <div class="comparison-card">
                        <h4>G-Retriever Light (Untrainiert)</h4>
                        <ul>
                            <li>Sofort einsatzbereit</li>
                            <li>Keine GPU erforderlich</li>
                            <li>Schnelle Antworten</li>
                            <li>Embedding-basiertes Retrieval</li>
                            <li>PCST Subgraph Construction</li>
                            <li>Ollama f√ºr Answer Generation</li>
                        </ul>
                        <div class="success-box">
                            <strong>Empfehlung:</strong> Start hiermit! Funktioniert sehr gut ohne Training.
                        </div>
                    </div>

                    <div class="comparison-card">
                        <h4>G-Retriever Full (Trainiert)</h4>
                        <ul>
                            <li>Erfordert Training (1-3h)</li>
                            <li>GPU empfohlen</li>
                            <li>Spezialisiert auf deine Daten</li>
                            <li>GNN-basiertes Retrieval</li>
                            <li>Graph Attention Networks</li>
                            <li>5-10% bessere Ergebnisse</li>
                        </ul>
                        <div class="warning-box">
                            <strong>Hinweis:</strong> Nur f√ºr Enthusiasten oder gro√üe Vaults (>5000 Notizen) n√∂tig.
                        </div>
                    </div>
                </div>

                <h3>Kernkomponenten:</h3>

                <div class="module-card">
                    <h4>1. Graph Neural Network (GAT)</h4>
                    <p>Nutzt Graph Attention Networks um Beziehungen zwischen Notizen zu lernen. Mit 3 Layers und 4 Attention Heads kann das Modell komplexe Verbindungsmuster erkennen.</p>
                </div>

                <div class="module-card">
                    <h4>2. Sentence Transformers</h4>
                    <p>Erstellt semantische Embeddings f√ºr alle Notizen. Das Model <code>all-MiniLM-L6-v2</code> ist schnell und effizient mit 384-dimensionalen Vektoren.</p>
                </div>

                <div class="module-card">
                    <h4>3. PCST Algorithm</h4>
                    <p>Prize-Collecting Steiner Tree findet den optimal verbundenen Subgraph aus relevanten Knoten - essenziell f√ºr koh√§rente Antworten.</p>
                </div>

                <div class="module-card">
                    <h4>4. Ollama LLM</h4>
                    <p>Dein lokales Llama3 Model generiert die finalen Antworten basierend auf dem retrieval Context. Vollst√§ndige Privatsph√§re, keine Cloud!</p>
                </div>
            </section>

            <section id="installation">
                <h2>‚öôÔ∏è Installation</h2>

                <h3>Voraussetzungen:</h3>
                <ul>
                    <li>Python 3.9 oder h√∂her</li>
                    <li>CUDA (optional, f√ºr GPU-Beschleunigung)</li>
                    <li>Ollama installiert mit <code>llama3:8b</code> Model</li>
                    <li>Ca. 10 GB freier Speicherplatz</li>
                </ul>

                <h3>Schritt 1: Virtual Environment</h3>
                <pre><code>python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows</code></pre>

                <h3>Schritt 2: PyTorch installieren</h3>
                <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code></pre>

                <h3>Schritt 3: PyTorch Geometric</h3>
                <pre><code>pip install torch-geometric
pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html</code></pre>

                <h3>Schritt 4: Weitere Dependencies</h3>
                <pre><code>pip install sentence-transformers networkx pcst-fast requests tqdm numpy pandas</code></pre>

                <h3>Schritt 5: Ollama einrichten</h3>
                <pre><code># Pr√ºfen ob Ollama l√§uft
curl http://localhost:11434/api/version

# Llama3 Model holen
ollama pull llama3:8b</code></pre>

                <div class="success-box">
                    <strong>‚úì Installation komplett!</strong> Du bist bereit loszulegen.
                </div>
            </section>

            <section id="modules">
                <h2>üì¶ Module</h2>

                <div class="module-card">
                    <h4>1. obsidian_to_graph.py</h4>
                    <p><strong>Funktion:</strong> Konvertiert Obsidian Vault in NetworkX Graph</p>
                    <p><strong>Input:</strong> Pfad zum Vault</p>
                    <p><strong>Output:</strong> <code>graph.gpickle</code>, <code>graph.json</code>, <code>stats.json</code></p>
                    <p><strong>Features:</strong></p>
                    <ul>
                        <li>Parst Markdown-Dateien</li>
                        <li>Extrahiert Wiki-Links [[link]] und Markdown-Links</li>
                        <li>Entfernt Bilder automatisch</li>
                        <li>Extrahiert #tags</li>
                        <li>Erstellt gerichteten Graph mit Kanten f√ºr Links</li>
                    </ul>
                </div>

                <div class="module-card">
                    <h4>2. generate_training_data.py</h4>
                    <p><strong>Funktion:</strong> Generiert QA-Paare mit Ollama</p>
                    <p><strong>Input:</strong> <code>graph.gpickle</code></p>
                    <p><strong>Output:</strong> <code>train.json</code>, <code>val.json</code>, <code>qa_pairs.json</code></p>
                    <p><strong>Fragetypen:</strong></p>
                    <ul>
                        <li><strong>Factual:</strong> Pr√§zise Faktenfragen</li>
                        <li><strong>Connection:</strong> Fragen √ºber Zusammenh√§nge</li>
                        <li><strong>Summary:</strong> Zusammenfassungsfragen</li>
                        <li><strong>Multi-Node:</strong> Fragen √ºber mehrere verbundene Notizen</li>
                    </ul>
                    <p><strong>Performance:</strong> ~500 QA-Paare in 1-2 Stunden</p>
                </div>

                <div class="module-card">
                    <h4>3. pyg_dataset.py</h4>
                    <p><strong>Funktion:</strong> Erstellt PyTorch Geometric Datasets</p>
                    <p><strong>Input:</strong> Graph + QA-JSONs</p>
                    <p><strong>Output:</strong> <code>train_data.pt</code>, <code>val_data.pt</code></p>
                    <p><strong>Features:</strong></p>
                    <ul>
                        <li>Node Embeddings mit Sentence Transformers</li>
                        <li>Question Embeddings</li>
                        <li>Edge Index f√ºr GNN</li>
                        <li>80/20 Train/Val Split</li>
                    </ul>
                </div>

                <div class="module-card">
                    <h4>4. gretriever_inference.py</h4>
                    <p><strong>Funktion:</strong> Chat Interface (untrainiert)</p>
                    <p><strong>Pipeline:</strong></p>
                    <ol>
                        <li><strong>Retrieval:</strong> k-NN mit Cosine Similarity</li>
                        <li><strong>Subgraph Construction:</strong> PCST f√ºr optimalen Subgraph</li>
                        <li><strong>Answer Generation:</strong> Ollama mit Context</li>
                    </ol>
                    <p><strong>Vorteil:</strong> Sofort einsatzbereit, kein Training n√∂tig!</p>
                </div>

                <div class="module-card">
                    <h4>5. train_gretriever.py</h4>
                    <p><strong>Funktion:</strong> Trainiert GNN auf QA-Paaren</p>
                    <p><strong>Model:</strong> GAT (Graph Attention Network)</p>
                    <p><strong>Loss:</strong> Binary Cross Entropy (relevante vs. irrelevante Knoten)</p>
                    <p><strong>Optimizer:</strong> Adam mit Learning Rate 0.001</p>
                    <p><strong>Training:</strong> 20 Epochen, ~1-3 Stunden</p>
                </div>

                <div class="module-card">
                    <h4>6. gretriever_inference_trained.py</h4>
                    <p><strong>Funktion:</strong> Chat Interface mit trainiertem GNN</p>
                    <p><strong>Unterschied:</strong> Nutzt trainiertes Model f√ºr Retrieval statt Embeddings</p>
                    <p><strong>Performance:</strong> 5-10% bessere Relevanz bei gro√üen Vaults</p>
                </div>

                <div class="module-card">
                    <h4>7. pipeline.py</h4>
                    <p><strong>Funktion:</strong> F√ºhrt komplette Pipeline automatisch aus</p>
                    <p><strong>Optionen:</strong> Einzelne Schritte √ºberspringen mit <code>--skip</code></p>
                    <p><strong>Perfekt f√ºr:</strong> Erste Einrichtung oder Neustart</p>
                </div>
            </section>

            <section id="workflow">
                <h2>üîÑ Workflow</h2>

                <h3>Quick Start (Untrainierte Variante):</h3>

                <div class="pipeline-step">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h4>Graph erstellen</h4>
                        <pre><code>python obsidian_to_graph.py</code></pre>
                        <p>Konvertiert deine Notizen in einen Graphen. Dauert: ~1-5 Minuten f√ºr 1100 Notizen.</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h4>Trainingsdaten generieren</h4>
                        <pre><code>python generate_training_data.py</code></pre>
                        <p>Erstellt 500 QA-Paare mit Ollama. Dauert: 1-2 Stunden.</p>
                        <div class="warning-box">
                            <strong>Tipp:</strong> Starte mit 200 QA-Paaren zum Testen (<code>num_samples=200</code>), dann erweitere auf 500-1000.
                        </div>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h4>Chat starten</h4>
                        <pre><code>python gretriever_inference.py</code></pre>
                        <p>Interaktives Chat-Interface √∂ffnet sich. Stelle Fragen √ºber deine Notizen!</p>
                    </div>
                </div>

                <h3>Erweitert (Trainierte Variante):</h3>

                <div class="pipeline-step">
                    <div class="step-number">4</div>
                    <div class="step-content">
                        <h4>PyG Dataset erstellen</h4>
                        <pre><code>python pyg_dataset.py</code></pre>
                        <p>Konvertiert Daten ins PyTorch Geometric Format. Dauert: 5-10 Minuten.</p>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">5</div>
                    <div class="step-content">
                        <h4>GNN Training</h4>
                        <pre><code>python train_gretriever.py</code></pre>
                        <p>Trainiert das Graph Neural Network. Dauert: 1-3 Stunden je nach Hardware.</p>
                        <div class="info-box">
                            <strong>GPU-Tipp:</strong> Mit GPU 3-5x schneller. CPU ist aber auch okay!
                        </div>
                    </div>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">6</div>
                    <div class="step-content">
                        <h4>Mit trainiertem Model chatten</h4>
                        <pre><code>python gretriever_inference_trained.py</code></pre>
                        <p>Nutzt das trainierte Model f√ºr besseres Retrieval.</p>
                    </div>
                </div>
            </section>

            <section id="training">
                <h2>üéì Training Details</h2>

                <h3>Wie viele Trainingsdaten brauchst du?</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Vault-Gr√∂√üe</th>
                            <th>Empfohlene QA-Paare</th>
                            <th>Dauer</th>
                            <th>Zweck</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>< 500 Notizen</td>
                            <td>200-300</td>
                            <td>30-60 min</td>
                            <td>Quick Test</td>
                        </tr>
                        <tr>
                            <td>500-1500 Notizen</td>
                            <td>500-800</td>
                            <td>1-2 h</td>
                            <td>Standard (empfohlen)</td>
                        </tr>
                        <tr>
                            <td>1500-3000 Notizen</td>
                            <td>1000-1500</td>
                            <td>3-4 h</td>
                            <td>Gute Abdeckung</td>
                        </tr>
                        <tr>
                            <td>> 3000 Notizen</td>
                            <td>2000+</td>
                            <td>6+ h</td>
                            <td>Sehr gut</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Training Hyperparameter:</h3>

                <div class="module-card">
                    <h4>Model Architektur</h4>
                    <ul>
                        <li><strong>Node Embed Dim:</strong> 384 (von Sentence Transformer)</li>
                        <li><strong>Hidden Dim:</strong> 256</li>
                        <li><strong>Num Layers:</strong> 3</li>
                        <li><strong>Attention Heads:</strong> 4</li>
                        <li><strong>Total Parameters:</strong> ~2.5M</li>
                    </ul>
                </div>

                <div class="module-card">
                    <h4>Training Setup</h4>
                    <ul>
                        <li><strong>Optimizer:</strong> Adam</li>
                        <li><strong>Learning Rate:</strong> 0.001</li>
                        <li><strong>Loss Function:</strong> BCE with Logits</li>
                        <li><strong>Epochs:</strong> 20 (default)</li>
                        <li><strong>Batch Size:</strong> 1 (full graph per sample)</li>
                    </ul>
                </div>

                <div class="info-box">
                    <h4>üí° Training Tipps:</h4>
                    <ul>
                        <li>Starte mit weniger Epochen (10) zum Testen</li>
                        <li>Monitore Validation Loss - bei Overfitting fr√ºher stoppen</li>
                        <li>Best Model wird automatisch gespeichert</li>
                        <li>Training History wird als JSON exportiert</li>
                    </ul>
                </div>
            </section>

            <section id="usage">
                <h2>üöÄ Nutzung</h2>

                <h3>Beispiel Chat Session:</h3>

                <pre><code>$ python gretriever_inference.py

============================================================
G-Retriever Chat Interface f√ºr Obsidian Vault
Tippe 'quit' oder 'exit' zum Beenden
============================================================

Deine Frage: Was sind die wichtigsten Konzepte in meinen ML Notizen?
Query: Was sind die wichtigsten Konzepte in meinen ML Notizen?
Retrieving relevant nodes...
Constructing subgraph...
Generating answer...
Antwort: Basierend auf deinen Notizen sind die wichtigsten Machine Learning
Konzepte: Neural Networks mit Backpropagation, Gradient Descent f√ºr die
Optimierung, verschiedene Loss Functions (MSE, Cross-Entropy), und
Regularisierung durch L1/L2. Du hast auch ausf√ºhrliche Notizen √ºber
Convolutional Neural Networks und ihre Anwendung in Computer Vision.
Verwendete Notizen: Neural Networks, Backpropagation, Gradient Descent,
Loss Functions, Regularization</code></pre>             <h3>Beispielabfragen:</h3>

            <div class="module-card">
                <h4>üîç Faktenfragen</h4>
                <ul>
                    <li>"Was ist der Unterschied zwischen L1 und L2 Regularisierung?"</li>
                    <li>"Welche Python Libraries nutze ich f√ºr Data Science?"</li>
                    <li>"Was steht in meiner Notiz √ºber Transformer?"</li>
                </ul>
            </div>

            <div class="module-card">
                <h4>üîó Zusammenhangsfragen</h4>
                <ul>
                    <li>"Wie h√§ngen meine Notizen √ºber GraphQL und REST APIs zusammen?"</li>
                    <li>"Welche Projekte nutzen React?"</li>
                    <li>"Was sind die Verbindungen zwischen meinen Psychologie-Notizen?"</li>
                </ul>
            </div>

            <div class="module-card">
                <h4>üìä Zusammenfassungen</h4>
                <ul>
                    <li>"Fasse meine Notizen √ºber Quantum Computing zusammen"</li>
                    <li>"Was habe ich √ºber Produktivit√§t gelernt?"</li>
                    <li>"√úberblick √ºber meine Reise-Notizen zu Japan"</li>
                </ul>
            </div>

            <h3>Code-Anpassungen:</h3>

            <h4>Pfade in den Modulen anpassen:</h4>

            <pre><code># In obsidian_to_graph.py
vault_path = "/pfad/zu/deinem/vault"
output_path = "./graph_output"
In generate_training_data.py
graph_path = "./graph_output/graph.gpickle"
output_path = "./training_data"
num_samples = 500  # Anzahl QA-Paare
In gretriever_inference.py
graph_path = "./graph_output/graph.gpickle"
ollama_model = "llama3:8b"</code></pre>
</section>
        <section id="comparison">
            <h2>‚öñÔ∏è Untrainiert vs. Trainiert</h2>

            <h3>Performance-Vergleich:</h3>

            <table>
                <thead>
                    <tr>
                        <th>Aspekt</th>
                        <th>Untrainiert (Light)</th>
                        <th>Trainiert (Full)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Setup-Zeit</strong></td>
                        <td>1-2 Stunden</td>
                        <td>3-5 Stunden</td>
                    </tr>
                    <tr>
                        <td><strong>GPU erforderlich?</strong></td>
                        <td>‚ùå Nein</td>
                        <td>‚ö†Ô∏è Empfohlen</td>
                    </tr>
                    <tr>
                        <td><strong>Retrieval-Qualit√§t</strong></td>
                        <td>85-90%</td>
                        <td>90-95%</td>
                    </tr>
                    <tr>
                        <td><strong>Antwortgeschwindigkeit</strong></td>
                        <td>2-5 Sekunden</td>
                        <td>3-6 Sekunden</td>
                    </tr>
                    <tr>
                        <td><strong>Vault-Gr√∂√üe Empfehlung</strong></td>
                        <td>< 2000 Notizen</td>
                        <td>> 2000 Notizen</td>
                    </tr>
                    <tr>
                        <td><strong>Wartung</strong></td>
                        <td>Keine</td>
                        <td>Re-training bei gro√üen √Ñnderungen</td>
                    </tr>
                    <tr>
                        <td><strong>Speicherbedarf</strong></td>
                        <td>~2 GB RAM</td>
                        <td>~4 GB RAM + 2 GB VRAM</td>
                    </tr>
                </tbody>
            </table>

            <div class="success-box">
                <h4>‚ú® Empfehlung:</h4>
                <p><strong>Starte mit der untrainierten Variante!</strong> Sie ist schnell eingerichtet, funktioniert exzellent und du kannst sofort loslegen. Trainiere nur, wenn:</p>
                <ul>
                    <li>Du mehr als 2000-3000 Notizen hast</li>
                    <li>Du die absolute beste Retrieval-Qualit√§t brauchst</li>
                    <li>Du Spa√ü am Experimentieren hast</li>
                </ul>
                <p>Die Qualit√§tsverbesserung durch Training ist marginal (5-10%), aber der Aufwand deutlich h√∂her.</p>
            </div>
        </section>

        <section id="troubleshooting">
            <h2>üîß Troubleshooting</h2>

            <div class="warning-box">
                <h4>Problem: Ollama Connection Error</h4>
                <p><strong>L√∂sung:</strong></p>
                <pre><code># Pr√ºfe ob Ollama l√§uft
curl http://localhost:11434/api/version
Starte Ollama
ollama serve</code></pre>
</div>
            <div class="warning-box">
                <h4>Problem: CUDA Out of Memory</h4>
                <p><strong>L√∂sung:</strong></p>
                <pre><code># In gretriever_inference.py oder train_gretriever.py
device = "cpu"  # Statt "cuda"</code></pre>
</div>
            <div class="warning-box">
                <h4>Problem: Zu wenig QA-Paare generiert</h4>
                <p><strong>Ursachen:</strong></p>
                <ul>
                    <li>Viele Notizen sind zu kurz (< 100 Zeichen)</li>
                    <li>JSON Parsing schl√§gt fehl</li>
                    <li>Ollama Timeouts</li>
                </ul>
                <p><strong>L√∂sung:</strong> Erh√∂he <code>num_samples</code> um 20-30% mehr als gew√ºnscht.</p>
            </div>

            <div class="warning-box">
                <h4>Problem: Import Errors</h4>
                <p><strong>L√∂sung:</strong></p>
                <pre><code># Reinstall dependencies
pip install --force-reinstall torch-geometric
pip install pyg-lib torch-scatter torch-sparse</code></pre>
</div>
            <div class="warning-box">
                <h4>Problem: Training sehr langsam</h4>
                <p><strong>Optimierungen:</strong></p>
                <ul>
                    <li>Nutze GPU statt CPU</li>
                    <li>Reduziere Hidden Dim auf 128</li>
                    <li>Reduziere Num Layers auf 2</li>
                    <li>Nutze weniger QA-Paare f√ºr ersten Test</li>
                </ul>
            </div>
        </section>

        <section id="advanced">
            <h2>üéØ Erweiterte Konfiguration</h2>

            <h3>Embedding Models √§ndern:</h3>

            <div class="module-card">
                <pre><code># Bessere Qualit√§t (langsamer)
embedding_model = "all-mpnet-base-v2"
Multilingual
embedding_model = "paraphrase-multilingual-MiniLM-L12-v2"
Spezialisiert auf Code
embedding_model = "microsoft/codebert-base"</code></pre>
</div>
            <h3>GNN Architektur tunen:</h3>

            <div class="module-card">
                <pre><code># Mehr Kapazit√§t
hidden_dim = 512
num_layers = 5
num_heads = 8
Schneller, weniger Kapazit√§t
hidden_dim = 128
num_layers = 2
num_heads = 2</code></pre>
</div>
            <h3>Retrieval-Parameter:</h3>

            <div class="module-card">
                <pre><code># In gretriever_inference.py
Mehr Kontext
k_retrieve = 30  # Statt 20
Gr√∂√üerer Subgraph
max_subgraph_size = 20  # In construct_subgraph_pcst
Mehr Notizen im LLM Context
max_context_nodes = 15  # In generate_answer</code></pre>
</div>
            <h3>Ollama Model wechseln:</h3>

            <div class="module-card">
                <pre><code># Gr√∂√üeres Model (bessere Qualit√§t)
ollama_model = "llama3:70b"
Schnelleres Model
ollama_model = "phi3:mini"
Spezialisiert
ollama_model = "codellama:13b"  # F√ºr Code-Heavy Vaults</code></pre>
</div>

            <div class="module-card">
    <h4>‚ö†Ô∏è PCST Verhalten: Selektion, kein Expansion</h4>

    <p>
        Der Prize-Collecting Steiner Tree (PCST) Schritt erweitert die
        Menge der abgerufenen Knoten <strong>nicht</strong>. Er
        optimiert global und w√§hlt eine <strong>strukturoptimal</strong>e
        Teilmenge aus.
    </p>

    <div class="warning-box">
        <strong>Wichtig:</strong><br>
        Ein retrieved Node ist <em>nicht garantiert</em> im finalen Subgraphen.
        Retrieval liefert Kandidaten ‚Äî PCST entscheidet, welche davon sinnvoll sind.
    </div>

    <p>
        Im aktuellen Code wird PCST wie folgt aufgerufen:
    </p>

    <pre><code>vertices, _ = pcst_fast(
    edges,
    prizes,
    costs,
    root,
    1,
    1,
    'strong'
)</code></pre>

    <h4>Wie PCST Entscheidungen trifft</h4>

    <p><strong>1. Node Prizes</strong></p>
    <pre><code>prizes[relevante_nodes] = similarities[relevante_nodes]</code></pre>
    <ul>
        <li>Nur abgerufene Knoten erhalten einen Prize &gt; 0</li>
        <li>Alle anderen Knoten starten mit Prize = 0</li>
        <li>Ein retrieved Node ist <em>optional</em>, nicht verpflichtend</li>
    </ul>

    <p><strong>2. Kantengeb√ºhren</strong></p>
    <pre><code>costs = np.ones(edges.shape[0])</code></pre>
    <ul>
        <li>Jede Kante kostet 1</li>
        <li>Lange oder schwach verbundene Pfade sind teuer</li>
    </ul>

    <p><strong>3. Optimierungs-Kriterium</strong></p>
    <pre><code>behalte Knoten, wenn:  prize(knoten) ‚â• sum(edge_costs um ihn zu verbinden)</code></pre>

    <ul>
        <li>Hohe Similarity + kurze Distanz ‚Üí bleibt</li>
        <li>Mittlere Similarity + viele Hops ‚Üí f√§llt raus</li>
        <li>Niedrige Similarity + starke Vernetzung ‚Üí bleibt oft</li>
    </ul>

    <div class="info-box">
        <strong>Formal:</strong><br>
        <code>subgraph ‚äÜ retrieved_nodes ‚à™ connector_nodes</code><br>
        PCST garantiert niemals, dass alle retrieved Nodes drin sind.
    </div>

    <h4>Warum der Subgraph kleiner ist als Retrieval</h4>
    <ul>
        <li>Retrieved Nodes k√∂nnen thematisch verstreut sein</li>
        <li>Verbindungskosten √ºberwiegen manchmal die Relevanz</li>
        <li>Gut vernetzte Knoten werden bevorzugt</li>
    </ul>

    <p>
        Daher bleiben z.‚ÄØB. stark vernetzte Autoren oder Konzepte im Subgraphen,
        w√§hrend isolierte, aber relevante Notes entfernt werden.
    </p>

    <h4>Wie man das Verhalten von PCST beeinflussen kann</h4>

    <p>
        Du kannst steuern, wie selektiv PCST vorgeht:
    </p>

    <pre><code># Option A: Prizes erh√∂hen (mehr retrieved Nodes behalten)
prizes[relevante_nodes] = similarities[relevante_nodes] * 100

# Option B: Kantengeb√ºhren reduzieren (gr√∂√üere zusammenh√§ngende Subgraphs bevorzugen)
costs = np.full(edges.shape[0], 0.01)

# Option C: PCST komplett umgehen (nur Top-K Retrieval)
subgraph_nodes = relevante_nodes</code></pre>

    <div class="success-box">
        <strong>Zusammenfassung:</strong><br>
        PCST ist ein <em>Filter-Mechanismus</em>, der den
        strukturell sinnvollsten Kern ausw√§hlt ‚Äî keine Erweiterung.
        Unterschiede zwischen Retrieval-Output und finalem Subgraphen
        sind erwartungsgem√§√ü und korrekt.
    </div>
</div>

</section>

        <section id="performance">
            <h2>üìà Performance-Optimierung</h2>

            <h3>F√ºr gro√üe Vaults (>5000 Notizen):</h3>

            <div class="info-box">
                <h4>1. Node Embedding Caching</h4>
                <p>Pre-compute und speichere Embeddings separat:</p>
                <pre><code>import pickle
Nach erstem Run
with open('node_embeddings.pkl', 'wb') as f:
pickle.dump(self.node_embeddings, f)
In nachfolgenden Runs laden
with open('node_embeddings.pkl', 'rb') as f:
self.node_embeddings = pickle.load(f)</code></pre>
</div>
            <div class="info-box">
                <h4>2. Batch Processing f√ºr QA-Generierung</h4>
                <p>Nutze gr√∂√üere Batches:</p>
                <pre><code># In generate_training_data.py
batch_size = 10  # Mehrere Prompts parallel</code></pre>
</div>
            <div class="info-box">
                <h4>3. Graph-Pruning</h4>
                <p>Entferne isolierte Knoten:</p>
                <pre><code># Nach graph.build()
isolated = list(nx.isolates(self.graph))
self.graph.remove_nodes_from(isolated)</code></pre>
</div>
                <h3>Benchmark (1100 nodes):</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Operation</th>
                            <th>CPU (M2)</th>
                            <th>GPU (A100)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Graph Building</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                        <tr>
                            <td>Node Embeddings</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                        <tr>
                            <td>500 QA pairs</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                        <tr>
                            <td>PyG Dataset</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                        <tr>
                            <td>Training (tbd epochs)</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                        <tr>
                            <td>Query Inference</td>
                            <td>tbd</td>
                            <td>tbd</td>
                        </tr>
                    </tbody>
                </table>
        </section>

        <section id="faq">
            <h2>‚ùì FAQ</h2>

            <div class="module-card">
                <h4>Kann ich andere LLMs nutzen statt Ollama?</h4>
                <p>Ja! Du kannst <code>generate_answer()</code> anpassen um OpenAI, Anthropic oder andere APIs zu nutzen. Ollama ist nur die datenschutzfreundliche Default-Option.</p>
            </div>

            <div class="module-card">
                <h4>Funktioniert das auch mit anderen Note-Taking Apps?</h4>
                <p>Grunds√§tzlich ja! Du musst nur <code>obsidian_to_graph.py</code> anpassen um das spezifische Format zu parsen (z.B. Notion, Roam Research).</p>
            </div>

            <div class="module-card">
                <h4>Wie halte ich das System aktuell wenn ich neue Notizen schreibe?</h4>
                <p>F√ºhre einfach die Pipeline nochmal aus. F√ºr inkrementelle Updates k√∂nntest du ein Script schreiben das nur neue/ge√§nderte Notizen verarbeitet.</p>
            </div>

            <div class="module-card">
                <h4>Kann ich mehrere Vaults gleichzeitig nutzen?</h4>
                <p>Ja! Erstelle f√ºr jeden Vault einen separaten Output-Ordner. Du kannst sogar mehrere Graphen im selben Chat Interface kombinieren.</p>
            </div>

            <div class="module-card">
                <h4>Werden meine Daten irgendwo hochgeladen?</h4>
                <p>Nein! Alles l√§uft lokal. Ollama ist lokal, die Embeddings sind lokal, das Training ist lokal. Totale Privatsph√§re.</p>
            </div>

            <div class="module-card">
                <h4>Funktioniert das System auch auf anderen Sprachen?</h4>
                <p>Ja! Nutze multilingual Embedding Models und stelle sicher dass dein Ollama Model die Sprache unterst√ºtzt. Llama3 funktioniert gut mit Deutsch, Franz√∂sisch, Spanisch, etc.</p>
            </div>
        </section>

        <section id="resources">
            <h2>üìö Ressourcen & Links</h2>

            <div class="module-card">
                <h4>Papers & Research</h4>
                <ul>
                    <li><a href="https://arxiv.org/abs/2402.07630" target="_blank">G-Retriever Paper (arXiv)</a></li>
                    <li><a href="https://pytorch-geometric.readthedocs.io/" target="_blank">PyTorch Geometric Documentation</a></li>
                    <li><a href="https://www.sbert.net/" target="_blank">Sentence Transformers</a></li>
                </ul>
            </div>

            <div class="module-card">
                <h4>Tools</h4>
                <ul>
                    <li><a href="https://ollama.ai/" target="_blank">Ollama - Lokale LLMs</a></li>
                    <li><a href="https://obsidian.md/" target="_blank">Obsidian Note-Taking</a></li>
                    <li><a href="https://networkx.org/" target="_blank">NetworkX</a></li>
                </ul>
            </div>

            <div class="module-card">
                <h4>Community</h4>
                <ul>
                    <li>PyTorch Geometric Discord</li>
                    <li>Obsidian Community Forum</li>
                    <li>r/LocalLLaMA auf Reddit</li>
                </ul>
            </div>
        </section>

        <section id="conclusion">
            <h2>üéâ Fazit</h2>

            <div class="success-box">
                <h3>Du hast jetzt ein vollst√§ndiges Graph-basiertes RAG-System!</h3>
                <p>Dieses System kombiniert modernste Technologien:</p>
                <ul>
                    <li>‚úÖ Graph Neural Networks f√ºr strukturiertes Wissen</li>
                    <li>‚úÖ Semantic Search mit Embeddings</li>
                    <li>‚úÖ Intelligentes Subgraph Construction (PCST)</li>
                    <li>‚úÖ Lokale LLMs f√ºr Privatsph√§re</li>
                    <li>‚úÖ Modularer, erweiterbarer Code</li>
                </ul>

                <p style="margin-top: 20px;"><strong>N√§chste Schritte:</strong></p>
                <ol>
                    <li>Starte mit der untrainierten Variante</li>
                    <li>Teste verschiedene Fragen</li>
                    <li>Generiere mehr QA-Paare wenn n√∂tig</li>
                    <li>Optional: Trainiere f√ºr bessere Ergebnisse</li>
                    <li>Experimentiere mit verschiedenen Models und Parametern</li>
                </ol>
            </div>

            <div style="text-align: center; margin: 40px 0;">
                <a href="#overview" class="btn">Zur√ºck nach oben</a>
            </div>
        </section>
    </main>

    <footer>
        <p><strong>G-Retriever f√ºr Obsidian</strong></p>
        <p>Ein modulares System f√ºr intelligente Wissensgraphen</p>
        <p style="margin-top: 20px; opacity: 0.8;">Erstellt mit PyTorch Geometric, Sentence Transformers & Ollama</p>
        <p style="margin-top: 10px; opacity: 0.6;">¬© 2024 - Open Source & Privacy First</p>
    </footer>
</div>
</body>
</html>
